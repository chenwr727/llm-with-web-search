# LLM-With-Web-Search

ä¸€ä¸ªåŸºäº LLM çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒç½‘ç»œæœç´¢å¢å¼ºçš„å¯¹è¯èƒ½åŠ›ã€‚

<div align="center">
  <img src="demo.png" alt="Demo">
</div>

## åŠŸèƒ½ç‰¹ç‚¹

- ğŸ¤– æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢
- ğŸŒ æ”¯æŒå¤šä¸ªæœç´¢å…³é”®è¯çš„å¹¶å‘æœç´¢
- ğŸ” æ™ºèƒ½æå–å’Œè¿‡æ»¤æœç´¢ç»“æœ
- ğŸ’¬ æ”¯æŒä¸Šä¸‹æ–‡å¯¹è¯
- âš¡ æµå¼è¾“å‡ºå“åº”
- ğŸ¯ æ”¯æŒæ€ç»´é“¾å’Œæ¨ç†è¿‡ç¨‹å±•ç¤º
- ğŸ”„ æ”¯æŒå¤šç§ LLM æ¨¡å‹ï¼ˆOpenAIã€DeepSeekç­‰ï¼‰
- ğŸ“± æä¾› Web UI ç•Œé¢

## å®‰è£…

1. å…‹éš†é¡¹ç›®
```bash
git clone https://gitlab.jinsubao.cn/chenwr/llm-with-web-search.git
cd LLM-With-Web-Search
```

2. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
playwright install chromium
```

3. é…ç½®ç¯å¢ƒå˜é‡
åˆ›å»º `.env` æ–‡ä»¶å¹¶é…ç½®ä»¥ä¸‹å‚æ•°ï¼š
```plaintext
# llm
ANALYSIS_LLM_API_KEY=your_analysis_llm_api_key
ANALYSIS_LLM_BASE_URL=your_analysis_llm_base_url
ANALYSIS_LLM_MODEL=your_analysis_llm_model
ANALYSIS_LLM_TEMPERATURE=0.6

ANSWER_LLM_API_KEY=your_answer_llm_api_key
ANSWER_LLM_BASE_URL=your_answer_llm_base_url
ANSWER_LLM_MODEL=your_answer_llm_model
ANSWER_LLM_TEMPERATURE=0.6

# search
BOCHA_API_KEY=your_bocha_api_key
BOCHA_NEEDS_CRAWLER=false
BOCHA_NEEDS_FILTER=false

# log
LOG_LEVEL=INFO
```

## ä½¿ç”¨æ–¹æ³•

### Web UI ç•Œé¢

è¿è¡Œ Web åº”ç”¨ï¼š
```bash
uvicorn api_server:app --port 8000 --worker uvicorn.workers.UvicornWorker --workers 1
streamlit run web_app.py
```

### ä»£ç è°ƒç”¨

```python
from clients.llm import OpenAILLMClient
from clients.search import BingSearchClient
from core.assistant import Assistant
from schemas.chat_message import ChatMessage

async def main():
    # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
    analysis_llm = OpenAILLMClient(
        api_key="your_api_key",
        base_url="your_base_url",
        model="your_model"
    )
    answer_llm = OpenAILLMClient(
        api_key="your_api_key",
        base_url="your_base_url",
        model="your_model"
    )
    
    # åˆå§‹åŒ–æœç´¢å®¢æˆ·ç«¯
    search_client = BingSearchClient(needs_filter=True)
    
    # åˆ›å»ºåŠ©æ‰‹å®ä¾‹
    assistant = Assistant(analysis_llm, answer_llm, search_client)
    
    # åˆ›å»ºå¯¹è¯æ¶ˆæ¯
    messages = [ChatMessage(role="user", content="å¡‘æ–™7042æ˜¯ä»€ä¹ˆï¼Ÿ")]
    
    # è·å–å›ç­”
    answer = await assistant.answer_question(messages)
    print(answer)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

## é¡¹ç›®ç»“æ„

```
LLM-With-Web-Search/
â”œâ”€â”€ api/                    # å®¢æˆ·ç«¯å®ç°
â”‚   â”œâ”€â”€ dependencies.py     # ä¾èµ–æ³¨å…¥
â”‚   â”œâ”€â”€ middleware.py       # ä¸­é—´ä»¶
â”‚   â”œâ”€â”€ models.py           # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ routers.py          # è·¯ç”±å®šä¹‰
â”‚   â””â”€â”€ services.py         # ä¸šåŠ¡é€»è¾‘å®ç°
â”œâ”€â”€ clients/                # å®¢æˆ·ç«¯å®ç°
â”‚   â”œâ”€â”€ base/               # åŸºç¡€æ¥å£å®šä¹‰
â”‚   â”œâ”€â”€ llm/                # LLM å®¢æˆ·ç«¯å®ç°
â”‚   â””â”€â”€ search/             # æœç´¢å®¢æˆ·ç«¯å®ç°
â”œâ”€â”€ core/                   # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
â”œâ”€â”€ schemas/                # æ•°æ®æ¨¡å‹å®šä¹‰
â”œâ”€â”€ utils/                  # å·¥å…·å‡½æ•°
â”œâ”€â”€ example.py              # ç¤ºä¾‹ä»£ç 
â”œâ”€â”€ api_server.py           # API æœåŠ¡ç«¯
â””â”€â”€ web_app.py              # Web åº”ç”¨å…¥å£
```

## ä¸»è¦ç‰¹æ€§è¯´æ˜

1. åŒ LLM æ¶æ„
   - Analysis LLMï¼šè´Ÿè´£åˆ†æé—®é¢˜ã€æå–æœç´¢å…³é”®è¯
   - Answer LLMï¼šè´Ÿè´£ç”Ÿæˆæœ€ç»ˆå›ç­”

2. æ™ºèƒ½æœç´¢
   - è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢
   - æ”¯æŒå¤šå…³é”®è¯å¹¶å‘æœç´¢
   - æ™ºèƒ½è¿‡æ»¤å’Œæå–ç›¸å…³å†…å®¹

3. æµå¼è¾“å‡º
   - æ”¯æŒæœç´¢è¿‡ç¨‹å®æ—¶å±•ç¤º
   - æ”¯æŒæ€ç»´é“¾å±•ç¤º
   - æ”¯æŒç­”æ¡ˆæµå¼ç”Ÿæˆ